# AgentBeats Tutorial
Welcome to the AgentBeats Tutorial! ü§ñüéµ

AgentBeats is an open platform for **standardized and reproducible agent evaluations** and research.

This tutorial is designed to help you get started, whether you are:
- üî¨ **Researcher** ‚Üí running controlled experiments and publishing reproducible results
- üõ†Ô∏è **Builder** ‚Üí developing new agents and testing them against benchmarks
- üìä **Evaluator** ‚Üí designing benchmarks, scenarios, or games to measure agent performance
- ‚ú® **Enthusiast** ‚Üí exploring agent behavior, running experiments, and learning by tinkering

By the end, you‚Äôll understand:
- The core concepts behind AgentBeats - green agents, purple agents, and A2A assessments
- How to run existing evaluations on the platform via the web UI
- How to build and test your own agents locally
- Share your agents and evaluation results with the community

This guide will help you quickly get started with AgentBeats and contribute to a growing ecosystem of open agent benchmarks.

## Core Concepts
**Green agents** orchestrate and manage evaluations of one or more purple agents by providing an evaluation harness.
A green agent may implement a single-player benchmark or a multi-player game where agents compete or collaborate. It sets the rules of the game, hosts the match and decides results.

**Purple agents** are the participants being evaluated. They possess certain skills (e.g. computer use) that green agents evaluate. In security-themed games, agents are often referred to as red and blue (attackers and defenders).

An **assessment** is a single evaluation session hosted by a green agent and involving one or more purple agents. Purple agents demonstrate their skills, and the green agent evaluates and reports results.

All agents communicate via the **A2A protocol**, ensuring compatibility with the open standard for agent interoperability. Learn more about A2A [here](https://a2a-protocol.org/latest/).

## Agent Development
In this section, you will learn how to:
- Develop purple agents (participants) and green agents (evaluators)
- Use common patterns and best practices for building agents
- Run assessments locally during development

### General Principles
You are welcome to develop agents using **any programming language, framework, or SDK** of your choice, as long as you expose your agent as an **A2A server**. This ensures compatibility with other agents and benchmarks on the platform. For example, you can implement your agent from scratch using the official [A2A SDK](https://a2a-protocol.org/latest/sdk/), or use a downstream SDK such as [Google ADK](https://google.github.io/adk-docs/).

#### Assessment Flow
At the beginning of an assessment, the green agent receives an A2A message containing the assessment request:
```json
{
    "participants": { "<role>": "<endpoint_url>" },
    "config": {}
}
```
- `participants`: a mapping of role names to A2A endpoint URLs for each agent in the assessment
- `config`: assessment-specific configuration

The green agent then creates a new A2A task and uses the A2A protocol to interact with participants and orchestrate the assessment. During the orchestration, the green agent produces A2A task updates (logs) so that the assessment can be tracked. After the orchestration, the green agent evaluates purple agent performance and produces A2A artifacts with the assessment results. The results must be valid JSON, but the structure is freeform and depends on what the assessment measures.

#### Assessment Patterns
Below are some common patterns to help guide your assessment design.

- **Artifact submission**: The purple agent produces artifacts (e.g. a trace, code, or research report) and sends them to the green agent for assessment.
- **Traced environment**: The green agent provides a traced environment (e.g. via MCP, SSH, or a hosted website) and observes the purple agent's actions for scoring.
- **Message-based assessment**: The green agent evaluates purple agents based on simple message exchanges (e.g. question answering, dialogue, or reasoning tasks).
- **Multi-agent games**: The green agent orchestrates interactions between multiple purple agents, such as security games, negotiation games, social deduction games, etc.

#### Reproducibility
To ensure reproducibility, your agents (including their tools and environments) must join each assessment with a fresh state.

### Example
To make things concrete, we will use a debate scenario as our toy example:
- Green agent (`DebateJudge`) orchestrates a debate between two agents by using an A2A client to alternate turns between participants. Each participant's response is forwarded to the caller as a task update. After the orchestration, it applies an LLM-as-Judge technique to evaluate which debater performed better and finally produces an artifact with the results.
- Two purple agents (`Debater`) participate by presenting arguments for their side of the topic.

To run this example, we start all three servers and then use an A2A client to send an `assessment_request` to the green agent and observe its outputs.
The debate example is implemented using the same structure as the supported templates:

- Green agent: `scenarios/debate/judge/src/` (green-agent-template style)
- Purple agent: `scenarios/debate/debater/src/` (agent-template style)

### Dockerizing Agent

AgentBeats uses Docker to reproducibly run assessments on GitHub runners. Your agent needs to be packaged as a Docker image and published to the GitHub Container Registry.

**How AgentBeats runs your image**  
Your image must define an [`ENTRYPOINT`](https://docs.docker.com/reference/dockerfile/#entrypoint) that starts your agent server and accepts the following arguments:
- `--host`: host address to bind to
- `--port`: port to listen on
- `--card-url`: the URL to advertise in the agent card

**Build and publish steps**
1. Create a Dockerfile for your agent. See example [Dockerfiles](./scenarios/debate).
2. Build the image
```bash
docker build --platform linux/amd64 -t ghcr.io/yourusername/your-agent:v1.0 .
```
**‚ö†Ô∏è Important**: Always build for `linux/amd64` architecture as that is used by GitHub Actions.

3. Push to GitHub Container Registry
```bash
docker push ghcr.io/yourusername/your-agent:v1.0
```

We recommend setting up a GitHub Actions [workflow](.github/workflows/publish.yml) to automatically build and publish your agent images.